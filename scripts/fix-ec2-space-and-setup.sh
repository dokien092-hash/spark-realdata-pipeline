#!/bin/bash
# Script fix dung lÆ°á»£ng vÃ  setup auto-start cho EC2

set -e

cd ~/spark-realdata-pipeline || exit 1

echo "ðŸ§¹ Dá»n dáº¹p Docker Ä‘á»ƒ giáº£i phÃ³ng dung lÆ°á»£ng..."
docker-compose down 2>/dev/null || true
docker system prune -a -f

echo ""
echo "ðŸ“ Táº¡o file .env (náº¿u chÆ°a cÃ³)..."
if [ ! -f .env ]; then
    cat > .env << 'EOF'
POLYGON_API_KEY=MKtaIeJgaIVQCxwr_HskC4NhLndLPZXR
ALPHA_VANTAGE_KEY=VWR51RQTVFTSBEL7
FINNHUB_API_KEY=d412e99r01qr2l0c96sgd412e99r01qr2l0c96t0
EOF
    echo "âœ… ÄÃ£ táº¡o .env"
else
    echo "âœ… .env Ä‘Ã£ tá»“n táº¡i"
fi

echo ""
echo "ðŸš€ Khá»Ÿi Ä‘á»™ng Docker Compose (khÃ´ng cÃ³ Jupyter)..."
docker-compose up -d

echo ""
echo "â³ Äá»£i containers khá»Ÿi Ä‘á»™ng (30 giÃ¢y)..."
sleep 30

echo ""
echo "ðŸ“Š Kiá»ƒm tra containers..."
docker-compose ps

echo ""
echo "ðŸ” Setup auto-start khi EC2 reboot..."
sudo tee /etc/systemd/system/docker-compose.service > /dev/null << 'SERVICEEOF'
[Unit]
Description=Docker Compose Application Service
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/ec2-user/spark-realdata-pipeline
ExecStart=/usr/local/bin/docker-compose up -d
ExecStop=/usr/local/bin/docker-compose down
User=ec2-user
Group=docker

[Install]
WantedBy=multi-user.target
SERVICEEOF

sudo systemctl daemon-reload
sudo systemctl enable docker-compose.service

echo "âœ… ÄÃ£ setup auto-start service"
echo ""

echo "ðŸ“‹ Logs Airflow Scheduler:"
docker-compose logs airflow-scheduler --tail 20

echo ""
echo "âœ… HoÃ n táº¥t!"
echo ""
echo "ðŸŒ Airflow UI: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8081"
echo "   Username: admin | Password: admin"
echo ""
echo "â° DAG sáº½ tá»± Ä‘á»™ng cháº¡y lÃºc 7:00 sÃ¡ng VN (0:00 UTC) tá»« T2-T6"
echo ""
echo "ðŸ“ Lá»‡nh há»¯u Ã­ch:"
echo "   - Xem logs: docker-compose logs airflow-scheduler --tail 50"
echo "   - Check DAG: docker-compose exec airflow-webserver airflow dags list"
echo "   - Restart: docker-compose restart"






