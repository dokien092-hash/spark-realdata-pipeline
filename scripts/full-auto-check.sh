#!/bin/bash
# Script ki·ªÉm tra to√†n b·ªô t·ª± ƒë·ªông ch·∫°y l√∫c 7h s√°ng

EC2_HOST="3.25.91.76"
EC2_USER="ec2-user"
KEY_PATH="$HOME/Downloads/financial-pipeline-key.pem"

echo "üîç KI·ªÇM TRA T·ª∞ ƒê·ªòNG CH·∫†Y L√öC 7H S√ÅNG"
echo "=========================================="
echo ""

ssh -i "$KEY_PATH" -o StrictHostKeyChecking=no -o ConnectTimeout=15 \
    "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
cd ~/spark-realdata-pipeline

echo "‚úÖ 1. Ki·ªÉm tra Docker containers ƒëang ch·∫°y:"
echo "--------------------------------------------"
docker-compose ps
echo ""

echo "‚úÖ 2. Ki·ªÉm tra Auto-start service (systemd):"
echo "--------------------------------------------"
if sudo systemctl is-enabled docker-compose.service 2>/dev/null; then
    echo "‚úÖ Auto-start ENABLED - Containers s·∫Ω t·ª± kh·ªüi ƒë·ªông khi EC2 reboot"
    sudo systemctl status docker-compose.service --no-pager | head -10
else
    echo "‚ö†Ô∏è  Auto-start NOT ENABLED - C·∫ßn setup systemd service"
    echo "   Ch·∫°y l·ªánh sau ƒë·ªÉ enable:"
    echo "   sudo systemctl enable docker-compose.service"
fi
echo ""

echo "‚úÖ 3. Ki·ªÉm tra Airflow DAG schedule:"
echo "--------------------------------------------"
echo "ƒê·ª£i 10 gi√¢y ƒë·ªÉ scheduler load DAG..."
sleep 10

DAG_INFO=$(docker-compose exec -T airflow-webserver airflow dags list 2>/dev/null | grep financial_pipeline_dag || \
    docker exec airflow-webserver airflow dags list 2>/dev/null | grep financial_pipeline_dag)

if [ -n "$DAG_INFO" ]; then
    echo "‚úÖ DAG found:"
    echo "$DAG_INFO"
    echo ""
    
    # Check DAG schedule
    SCHEDULE=$(docker-compose exec -T airflow-webserver airflow dags show financial_pipeline_dag 2>/dev/null | grep -i "schedule" || \
        docker exec airflow-webserver airflow dags show financial_pipeline_dag 2>/dev/null | grep -i "schedule")
    
    if echo "$SCHEDULE" | grep -q "0 0 \* \* 1-5\|0 0 1-5"; then
        echo "‚úÖ Schedule ƒë√∫ng: 0:00 UTC (7:00 AM VN) t·ª´ T2-T6"
    else
        echo "‚ö†Ô∏è  Schedule: $SCHEDULE"
        echo "   Ki·ªÉm tra schedule_interval trong DAG file"
    fi
    
    # Check DAG is unpaused
    IS_PAUSED=$(docker-compose exec -T airflow-webserver airflow dags list-runs -d financial_pipeline_dag --output table 2>/dev/null | grep -i pause || \
        docker exec airflow-webserver airflow dags list-runs -d financial_pipeline_dag --output table 2>/dev/null | grep -i pause)
    
    if echo "$IS_PAUSED" | grep -qi "false\|unpause"; then
        echo "‚úÖ DAG is UNPAUSED (s·∫Ω t·ª± ch·∫°y)"
    else
        echo "‚ö†Ô∏è  DAG c√≥ th·ªÉ b·ªã PAUSED - c·∫ßn unpause trong Airflow UI"
    fi
else
    echo "‚ö†Ô∏è  DAG ch∆∞a load - ki·ªÉm tra scheduler logs"
fi
echo ""

echo "‚úÖ 4. Ki·ªÉm tra Airflow Scheduler logs:"
echo "--------------------------------------------"
echo "Latest logs (last 20 lines):"
docker-compose logs airflow-scheduler --tail 20 2>/dev/null | tail -20
echo ""

echo "‚úÖ 5. Ki·ªÉm tra Database connection:"
echo "--------------------------------------------"
if docker-compose exec -T postgres pg_isready -U postgres 2>/dev/null | grep -q "accepting"; then
    echo "‚úÖ PostgreSQL ƒëang ch·∫°y v√† s·∫µn s√†ng"
    
    # Check if Airflow DB is initialized
    DB_EXISTS=$(docker-compose exec -T postgres psql -U postgres -d realdata_warehouse -tAc "SELECT 1 FROM information_schema.tables WHERE table_name='dag' LIMIT 1;" 2>/dev/null || echo "0")
    
    if [ "$DB_EXISTS" = "1" ]; then
        echo "‚úÖ Airflow database ƒë√£ ƒë∆∞·ª£c init"
    else
        echo "‚ö†Ô∏è  Airflow database ch∆∞a init - c·∫ßn ch·∫°y: docker exec airflow-scheduler airflow db init"
    fi
else
    echo "‚ö†Ô∏è  PostgreSQL kh√¥ng s·∫µn s√†ng"
fi
echo ""

echo "‚úÖ 6. Ki·ªÉm tra DAG runs g·∫ßn ƒë√¢y:"
echo "--------------------------------------------"
RECENT_RUNS=$(docker-compose exec -T airflow-webserver airflow dags list-runs -d financial_pipeline_dag --output table 2>/dev/null | head -15 || \
    docker exec airflow-webserver airflow dags list-runs -d financial_pipeline_dag --output table 2>/dev/null | head -15)

if [ -n "$RECENT_RUNS" ]; then
    echo "$RECENT_RUNS"
else
    echo "‚ö†Ô∏è  Kh√¥ng c√≥ runs n√†o - DAG c√≥ th·ªÉ ch∆∞a ƒë∆∞·ª£c trigger ho·∫∑c ch∆∞a ch·∫°y l·∫ßn n√†o"
fi
echo ""

echo "‚úÖ 7. Ki·ªÉm tra Environment variables (API keys):"
echo "--------------------------------------------"
if [ -f .env ]; then
    echo "‚úÖ File .env t·ªìn t·∫°i"
    if grep -q "FINNHUB_API_KEY" .env && ! grep -q "FINNHUB_API_KEY=$" .env; then
        echo "‚úÖ FINNHUB_API_KEY ƒë√£ ƒë∆∞·ª£c set"
    else
        echo "‚ö†Ô∏è  FINNHUB_API_KEY ch∆∞a ƒë∆∞·ª£c set trong .env"
    fi
else
    echo "‚ö†Ô∏è  File .env kh√¥ng t·ªìn t·∫°i"
fi
echo ""

echo "‚úÖ 8. T·ªïng k·∫øt:"
echo "--------------------------------------------"
echo "üìã ƒê·ªÉ ƒë·∫£m b·∫£o t·ª± ƒë·ªông ch·∫°y l√∫c 7h s√°ng (kh√¥ng c·∫ßn m·ªü m√°y):"
echo ""
echo "‚úì Docker containers ph·∫£i ch·∫°y (ƒë√£ check)"
echo "‚úì Auto-start service ph·∫£i ENABLED (ƒë√£ check)"
echo "‚úì Airflow DAG schedule = '0 0 * * 1-5' (ƒë√£ check)"
echo "‚úì DAG ph·∫£i UNPAUSED (ƒë√£ check)"
echo "‚úì Database ph·∫£i s·∫µn s√†ng (ƒë√£ check)"
echo ""
echo "üåê Truy c·∫≠p Airflow UI ƒë·ªÉ ki·ªÉm tra th√™m:"
PUBLIC_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4 2>/dev/null || echo "N/A")
echo "   http://${PUBLIC_IP}:8081"
echo "   Username: admin | Password: admin"
echo ""
echo "‚è∞ DAG s·∫Ω t·ª± ƒë·ªông ch·∫°y l√∫c 0:00 UTC (7:00 AM VN) t·ª´ Th·ª© 2 - Th·ª© 6"
echo "   L·∫ßn ch·∫°y ti·∫øp theo: $(date -u -d 'tomorrow 00:00' '+%Y-%m-%d %H:%M UTC' 2>/dev/null || date -u -v+1d -v0H -v0M '+%Y-%m-%d %H:%M UTC' 2>/dev/null || echo 'Tomorrow 00:00 UTC')"

REMOTE_SCRIPT

echo ""
echo "‚úÖ Ki·ªÉm tra ho√†n t·∫•t!"




