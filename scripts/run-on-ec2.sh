#!/bin/bash
# Script cháº¡y lá»‡nh trÃªn EC2 tá»« xa

EC2_HOST="3.25.91.76"
EC2_USER="ec2-user"
KEY_PATH="$HOME/Downloads/financial-pipeline-key.pem"

echo "ğŸ”Œ Connecting to EC2 and running command..."
echo ""

# Cháº¡y lá»‡nh fix Airflow DB
ssh -i "$KEY_PATH" -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
    "$EC2_USER@$EC2_HOST" << 'REMOTE_SCRIPT'
cd ~/spark-realdata-pipeline

echo "ğŸ”§ Fixing Airflow database initialization..."

# Check if containers are running
if ! docker-compose ps | grep -q "Up"; then
    echo "âš ï¸  Containers not running, starting..."
    docker-compose up -d
    sleep 20
fi

# Init database
echo "ğŸ“Š Initializing Airflow database..."
docker exec airflow-scheduler airflow db init 2>&1 | tail -10

# Restart scheduler
echo ""
echo "ğŸ”„ Restarting scheduler..."
docker-compose restart airflow-scheduler

# Wait and check logs
echo ""
echo "â³ Waiting 15 seconds..."
sleep 15

echo ""
echo "ğŸ“‹ Recent scheduler logs:"
docker-compose logs airflow-scheduler --tail 20

echo ""
echo "âœ… Check status:"
docker-compose ps

echo ""
echo "ğŸŒ Airflow UI: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4):8081"
REMOTE_SCRIPT

echo ""
echo "âœ… Command completed!"




