# ============================================================
# LỆNH CHẠY TRÊN EC2 (SAU KHI VÀO ĐƯỢC TERMINAL)
# Copy từng nhóm và paste vào terminal EC2
# ============================================================

# NHÓM 1: Kiểm tra project
cd ~/spark-realdata-pipeline || cd ~ && ls -la

# NHÓM 2: Tạo .env và start containers
cat > .env << 'EOF'
POLYGON_API_KEY=MKtaIeJgaIVQCxwr_HskC4NhLndLPZXR
ALPHA_VANTAGE_KEY=VWR51RQTVFTSBEL7
FINNHUB_API_KEY=d412e99r01qr2l0c96sgd412e99r01qr2l0c96t0
EOF

docker-compose down
docker-compose up -d
sleep 30
docker-compose ps

# NHÓM 3: Setup auto-start
sudo tee /etc/systemd/system/docker-compose.service > /dev/null << 'EOF'
[Unit]
Description=Docker Compose Application Service
Requires=docker.service
After=docker.service

[Service]
Type=oneshot
RemainAfterExit=yes
WorkingDirectory=/home/ec2-user/spark-realdata-pipeline
ExecStart=/usr/local/bin/docker-compose up -d
ExecStop=/usr/local/bin/docker-compose down
User=ec2-user
Group=docker

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable docker-compose.service
sudo systemctl is-enabled docker-compose.service

# NHÓM 4: Init Airflow
docker exec airflow-scheduler airflow db init
sleep 60

# NHÓM 5: Setup DAG
docker-compose exec airflow-webserver airflow dags list | grep financial
docker-compose exec airflow-webserver airflow dags unpause financial_pipeline_dag

# NHÓM 6: Check kết quả
docker-compose ps
echo "Airflow UI: http://3.25.91.76:8081"




